#!/usr/bin/env python
from tauperf.imaging.processing import process_taus

import os
import numpy as np
from sklearn import model_selection
from h5py import File
import tables

from tauperf import log; log = log['/tau-image']
from tauperf.parallel import run_pool, FuncWorker

def create_image_data(filename, tau_type, n_chunks=3, show_progress=False, outdir='.'):
    """
    """
    log.info('open h5 file {0}'.format(filename))

    #     h5file = File(filename, mode='r')
    h5file = tables.open_file(filename)
    dir_name = os.path.dirname(filename)

    #     records = h5file.get('tree_' + tau_type)
    records = getattr(h5file.root, 'tree_' + tau_type)


    out_name = os.path.join(outdir, 'images_new_{0}.h5'.format(tau_type))
    log.info('create h5 file {0}'.format(out_name))
    out_h5 = tables.open_file(out_name, mode='w')
    group = out_h5.create_group("/", 'data', 'yup')


    log.info('process {0}: {1} entries'.format(tau_type, len(records)))
    process_taus(
        out_h5, records, cal_layer=None, do_tracks=True, 
        suffix=tau_type, show_progress=show_progress)

    images = out_h5.root.data.images

    # use indices to split train / validation / test
    indices = xrange(len(images))
    train_ind, test_ind = model_selection.train_test_split(
        indices, test_size=0.2, random_state=42)
    val_ind, test_ind = np.split(test_ind, [len(test_ind) / 2])

    # chunk training
    train_ind = np.array_split(train_ind, n_chunks)

    for it, tr in enumerate(train_ind):
        h5file.create_table(group, 'train_{0}'.format(it), images[tr])

    out_h5.create_table(group, 'test', images[test_ind])
    out_h5.create_table(group, 'val', images[val_ind])

    # clear the file from the full set of images (split in train/val/test)
    out_h5.remove_node(group, 'images')
    log.info('close h5 file {0}'.format(out_name))
    out_h5.flush()
    out_h5.close()

    log.info('close h5 file {0}'.format(filename))
    h5file.flush()
    h5file.close()
    log.info('')
    


if __name__ == '__main__':
    from argparse import ArgumentParser
    parser = ArgumentParser()
    parser.add_argument('h5file', help='path to the h5 input file')
    parser.add_argument(
        '--tau-type', default='all', choices=['all', '1p0n', '1p1n', '1p2n', '3p0n', '3p1n'],
        help='choice of the type of taus')
    parser.add_argument('--no-progress-bar', default=False, action='store_true')
    parser.add_argument('--outdir', default=None)
    parser.add_argument('--chunks', default=3, type=int)
    parser.add_argument('--jobs', default=1, type=int)

    args = parser.parse_args()

    tau_types = ['1p0n', '1p1n', '1p2n', '3p0n', '3p1n']
    tau_type = args.tau_type
    
    h5_filename = args.h5file

    if args.outdir is None:
        outdir = os.path.dirname(os.path.abspath(args.h5file))
    else:
        outdir = args.outdir
    print outdir

    if tau_type == 'all':
        if args.jobs == 1:
            for tt in tau_types:
                create_image_data(
                    h5_filename, tt, 
                    n_chunks=args.chunks,
                    show_progress=not args.no_progress_bar, 
                    outdir=outdir)
        else:
            log.info('muliprocess this!')
            workers = []
            for tt in tau_types:
                w = FuncWorker(
                    create_image_data, h5_filename, tt, 
                    n_chunks=args.chunks, show_progress=False, outdir=outdir)
                workers.append(w)
            run_pool(workers, n_jobs=args.jobs)

    else:
        create_image_data(
            h5_filename, tau_type, 
            n_chunks=args.chunks,
            show_progress=not args.no_progress_bar, 
            outdir=outdir)
            
