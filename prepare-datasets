#!/usr/bin/env python
import os
import subprocess
import glob
import logging

from tauperf import UNMERGED_NTUPLE_PATH
from tauperf.datasets import create_database, read_database

log = logging.getLogger(os.path.basename(__file__))

from itertools import izip_longest
def grouper(iterable, n, fillvalue=None):
    "Collect data into fixed-length chunks or blocks"
    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx
    args = [iter(iterable)] * n
    return izip_longest(fillvalue=fillvalue, *args)


if __name__ == '__main__':
    from rootpy.extern.argparse import ArgumentParser
    parser = ArgumentParser()
    parser.add_argument('action', type=str, choices=['db', 'clean', 'merge', 'dev'])
    parser.add_argument('--key', type=str, choices=['corrected', 'weighted.corrected', 'extended'], default='weighted.corrected')
    parser.add_argument('--dry', action='store_true', default=False)
    args = parser.parse_args()
    print args

    # Create the database
    if args.action == 'db':
        create_database()

    # Cleaning block
    if args.action == 'clean':
        SAMPLES = read_database()
        print SAMPLES
        for key, sample in SAMPLES.items():
            log.info('clean up %s ...' % key)
            if sample.has_key('dirs'):
                for d in sample['dirs']:
                    for rfile in os.listdir(os.path.join(UNMERGED_NTUPLE_PATH, d)):
                        if args.key in rfile:
                            cmd = 'rm {0}'.format(
                                os.path.join(UNMERGED_NTUPLE_PATH, d, rfile))
                            log.info(cmd)
                            if not args.dry:
                                subprocess.call(cmd, shell=True)
            else:
                log.info('No directories for sample %s' % key)

    # merging block
    if args.action == 'merge':
        SAMPLES = read_database()
        for key, sample in SAMPLES.items():
            log.info('Merging %s ...' % key)
            if sample.has_key('dirs'):
                all_files = []
                for d in sample['dirs']:
                    files = glob.glob(
                        os.path.join(UNMERGED_NTUPLE_PATH, d, args.key + '*'))
                    all_files.extend(files)
                target_file = os.path.join(
                    UNMERGED_NTUPLE_PATH, '{0}.{1}.root'.format(args.key, key))
                input_list = ' '.join(all_files)
                
                input_chunks = [
                    filter(lambda a: a is not None, chunk) for chunk in grouper(all_files, 500)
                    ] 
                chunk_names = [
                    'chunk_{0}_{1}'.format(key, i) for i, _ in enumerate(input_chunks)
                    ]
                log.info(chunk_names)
                log.info(input_chunks)
                cmds = [
                    'hadd {0} {1}'.format(
                       c_n, ' '.join(c)) for c_n, c in zip(chunk_names, input_chunks)
                    ]
                cmd_final = 'hadd {0} {1} && rm {1}'.format(target_file, ' '.join(chunk_names))

                # cmd = 'hadd {0} {1}'.format(target_file, input_list)
                if not args.dry:
                    try:
                        for cmd in cmds:
                            subprocess.call(cmd, shell=True)
                        subprocess.call(cmd_final, shell=True)
                    except OSError:
                        print 'OSError !'
                else:
                    # log.info(cmds)
                    log.info(cmd_final)
                    # log.info(cmd)

    # bac a sable
    if args.action == 'dev':
        SAMPLES = read_database()
        for key, sample in SAMPLES.items():
            log.info('Submitting ...')
            if sample.has_key('dirs'):
                for d in sample['dirs']:
                    files = glob.glob(
                        os.path.join(UNMERGED_NTUPLE_PATH, d, sample['prefix'] + '*.root*'))
                    print files
                       
